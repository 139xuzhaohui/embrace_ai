
## Langchain

### Overview - how
this is  quickstart demonstrates **how to build a calculator agent using the LangGraph Graph API or the Functional API.**

- for **the LangGraph Graph API** 
- <img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/0192e991-c62c-4034-9464-1b0fa686d185" />

- for **the Functional API** 
- <img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/f93dd409-464b-45b4-92ba-309b98ecc090" />

### Get started Run a local server - how
https://docs.langchain.com/oss/python/langgraph/local-server

### Thinking in LangGraph
> When you build **an agent with LangGraph,**
>  1. you will first break it apart into discrete steps called nodes.
>  2. Then, you will describe the different decisions and transitions from each of your nodes.
>  3. Finally, you connect nodes together through **a shared state that each node can read from**  and write to.

> 先构建你的nodes，通过条件、转换 组织你的nodes， *最后将你想的用一个 a shared state 进行串联起来*

``` 
这里面就教你 如何在一个workflow里面 ，
- 1、首先llm node 、data 、action、 hunman --在这几个节点里 如何 去共享一个状态
- 2、遇到错误怎么办（Transient errors --临时性错误）对于不同的错误 -我们怎么应对
> 还有你看： The graph pauses when it hits interrupt(), ** saves everything to the checkpointer,得保留现场，如果被打断了**

```

### workflow 
- **workflow 、graph 、agent的各自差异**
- Workflows have **predetermined code paths**  and are **designed to operate in a certain order**. 【重点: 预先规划好的，被设计时是固定命令的】
- Agents are **dynamic and define their own processes and tool** usage. 【重点： 更多dynamic、flexible，我们补充tools】
> <img width="500" height="376" alt="image" src="https://github.com/user-attachments/assets/5333ad4a-4cb3-4985-a6fd-1339b1c5e538" />

#### workflow agents+ 的可实现的方式【for 如何让我的工具/agent越来越强】
- **增强技术点** - ways:
- 1. LLM 层面:
     - toolcalling -检索能力（向量库吧）
     - structured outputs, -- prompt吗？结构化输出？？
     - short memory -- 记忆
- 2. Prompt chaining --todo 说实话不是很懂
  3. 并行的
  4. 协凋的
  5. 优化的
  6. 路由的
  

### langMemory  

对于 **langchina** 来说 ， langMemory  主要是 long-term 和 short-term
保持记忆-会话的层次/维度 可以是

#### 1、long-tremMemory
> corss conversation 、need to store  -- use **InMemoryStore**
> lets your agents continuously improve -- 持续进化
> and  personalize their responses  -- 个性化响应
> keep maintain consistent behavior across sessions. 在跨对话中保持一致性


> "in the hot path" 热路径
> Native integration with LangGraph's Long-term Memory Store 原生集成

 - ways
 - 1. In production, use a store backed by a database
   2. Use semantic search -- 语义搜索,通过你的 **graph agent** 去进行 语义相近性 查询 ，about in your graph’s memory store
   > 也就是训练你自己的agent，让他自己去判断，--比如你说过你喜欢pizza，下次说我饿了 你有什么推荐呢--- 结合记忆他正常会给你pizza的答案，*但前提是*  **“你需要 store memory!!!!”**
   > Enable semantic search in your graph’s memory store to let **graph agents** search for items in the store by **semantic similarity**.

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Create store with semantic search enabled
embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)

```

#### 2、short-tremMemory
> 为什么有 long-tremMemory 还需要 short ， 个人理解 主要是因为 部分对话 是很临时性的 其次成本考虑 没必要每次 need remember
> 或者 在一个长对话中 long conversations might be **exceed the LLM’s context window.**
 - ways
 - 1. Trim messages ， Remove first or last N messages (before calling LLM) Delete messages from LangGraph state permanently， 在llm 持久化之前去手动remove，这里 是不是可以用LRU了
   2. Summarize messages == 理解就是 compacted message，合并整理信息， 其实可以理解起来 --像是压缩替代的 **replace** 的思想
   3. Manage checkpoints to store and retrieve message history == 检查点, 嗯... 也就是**存档吧** 【store and retrieve】 存储和检索！！
   4. Custom strategies --自己实现咯，maybe像一些 外部的 kvcache for llm Memory
   >   TODO 摘要 和 checkpoint 嗯... 可以考究一下


denominated in tokens -- 以令牌为单位
denominated 标记！！

##### 2.1.trim_messages 【裁剪】
- demo
> **在定义message中，定义strategy="last", max_tokens=**
```
from langchain_core.messages.utils import (  
    trim_messages,  
    count_tokens_approximately  
)  

def call_model(state: MessagesState):
    messages = trim_messages(  
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...

```
##### 2.2.Summarize messages 【摘要合并】
- demo
> **生成摘要**

```python
def summarize_conversation(state: State):
    # First, we get any existing summary
    summary = state.get("summary", "")

    # Create our summarization prompt
    if summary:
        # A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )
    else:
        summary_message = "Create a summary of the conversation above:"

    # Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
```

##### 2.3.Manage checkpoints 【管理安全点】
- 主要是以 config = {。。。。} 去配置识别, 里面有： *checkpoint_id / thread_id*
- 可以去 查看 thread_id 当前的状态 或者 view history of the thread 或者 delete 某个 thread_id
- 提供From  Graph 或者 FunctionAPI 或者 checkpointAPI


##### 3、其实Langchian 还提供一种叫 【Prebuilt memory tools】 预构建以及的方式
- for this : **https://langchain-ai.github.io/langmem/**

##### 4、db 自己去接实现  implementation of BaseCheckpointSaver or BaseStore to enable



