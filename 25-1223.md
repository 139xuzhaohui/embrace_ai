
### langMemory  

对于 **langchina** 来说 ， langMemory  主要是 long-term 和 short-term
保持记忆-会话的层次/维度 可以是

#### 1、long-tremMemory
> corss conversation 、need to store  -- use **InMemoryStore**
> lets your agents continuously improve -- 持续进化
> and  personalize their responses  -- 个性化响应
> keep maintain consistent behavior across sessions. 在跨对话中保持一致性


> "in the hot path" 热路径
> Native integration with LangGraph's Long-term Memory Store 原生集成

 - ways
 - 1. In production, use a store backed by a database
   2. Use semantic search -- 语义搜索,通过你的 **graph agent** 去进行 语义相近性 查询 ，about in your graph’s memory store
   > 也就是训练你自己的agent，让他自己去判断，--比如你说过你喜欢pizza，下次说我饿了 你有什么推荐呢--- 结合记忆他正常会给你pizza的答案，*但前提是*  **“你需要 store memory!!!!”**
   > Enable semantic search in your graph’s memory store to let **graph agents** search for items in the store by **semantic similarity**.

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Create store with semantic search enabled
embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)

```

#### 2、short-tremMemory
> 为什么有 long-tremMemory 还需要 short ， 个人理解 主要是因为 部分对话 是很临时性的 其次成本考虑 没必要每次 need remember
> 或者 在一个长对话中 long conversations might be **exceed the LLM’s context window.**
 - ways
 - 1. Trim messages ， Remove first or last N messages (before calling LLM) Delete messages from LangGraph state permanently， 在llm 持久化之前去手动remove，这里 是不是可以用LRU了
   2. Summarize messages == 理解就是 compacted message，合并整理信息， 其实可以理解起来 --像是压缩替代的 **replace** 的思想
   3. Manage checkpoints to store and retrieve message history == 检查点, 嗯... 也就是**存档吧** 【store and retrieve】 存储和检索！！
   4. Custom strategies --自己实现咯，maybe像一些 外部的 kvcache for llm Memory
   >   TODO 摘要 和 checkpoint 嗯... 可以考究一下


denominated in tokens -- 以令牌为单位
denominated 标记！！

1. demo
   > **在定义message中， 定义strategy="last",,  max_tokens**
```
from langchain_core.messages.utils import (  
    trim_messages,  
    count_tokens_approximately  
)  

def call_model(state: MessagesState):
    messages = trim_messages(  
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...

```



- 1、一次 --every time to delete/always clean
- 2、窗口级别，、一次对话 嗯... 就是多次/轮对话， 但是对话窗口没有变过， 这个应该是 thread_id 么？--还是？
- 3、cross 窗口 -- all memory all i need。--- 这个就是 summarsize 吧 
**这个可以 问下ai**


### langMemory  
https://langchain-ai.github.io/langmem/



delete
summarize
threade


